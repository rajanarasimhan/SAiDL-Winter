{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8FYEbyczRGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import array\n",
        "import gzip\n",
        "import os\n",
        "from os import path\n",
        "import struct\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "_DATA = \"/tmp/jax_example_data/\"\n",
        "\n",
        "\n",
        "def _download(url, filename):\n",
        "  \"\"\"Download a url to a file in the JAX data temp directory.\"\"\"\n",
        "  if not path.exists(_DATA):\n",
        "    os.makedirs(_DATA)\n",
        "  out_file = path.join(_DATA, filename)\n",
        "  if not path.isfile(out_file):\n",
        "    urllib.request.urlretrieve(url, out_file)\n",
        "    print(\"downloaded {} to {}\".format(url, _DATA))\n",
        "\n",
        "\n",
        "def _partial_flatten(x):\n",
        "  \"\"\"Flatten all but the first dimension of an ndarray.\"\"\"\n",
        "  return np.reshape(x, (x.shape[0], -1))\n",
        "\n",
        "\n",
        "def _one_hot(x, k, dtype=np.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return np.array(x[:, None] == np.arange(k), dtype)\n",
        "\n",
        "\n",
        "def mnist_raw():\n",
        "  \"\"\"Download and parse the raw MNIST dataset.\"\"\"\n",
        "  # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
        "  base_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
        "\n",
        "  def parse_labels(filename):\n",
        "    with gzip.open(filename, \"rb\") as fh:\n",
        "      _ = struct.unpack(\">II\", fh.read(8))\n",
        "      return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
        "\n",
        "  def parse_images(filename):\n",
        "    with gzip.open(filename, \"rb\") as fh:\n",
        "      _, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "      return np.array(array.array(\"B\", fh.read()),\n",
        "                      dtype=np.uint8).reshape(num_data, rows, cols)\n",
        "\n",
        "  for filename in [\"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\",\n",
        "                   \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"]:\n",
        "    _download(base_url + filename, filename)\n",
        "\n",
        "  train_images = parse_images(path.join(_DATA, \"train-images-idx3-ubyte.gz\"))\n",
        "  train_labels = parse_labels(path.join(_DATA, \"train-labels-idx1-ubyte.gz\"))\n",
        "  test_images = parse_images(path.join(_DATA, \"t10k-images-idx3-ubyte.gz\"))\n",
        "  test_labels = parse_labels(path.join(_DATA, \"t10k-labels-idx1-ubyte.gz\"))\n",
        "\n",
        "  return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "def mnist(permute_train=False):\n",
        "  \"\"\"Download, parse and process MNIST data to unit scale and one-hot labels.\"\"\"\n",
        "  train_images, train_labels, test_images, test_labels = mnist_raw()\n",
        "\n",
        "  train_images = _partial_flatten(train_images) / np.float32(255.)\n",
        "  test_images = _partial_flatten(test_images) / np.float32(255.)\n",
        "  train_labels = _one_hot(train_labels, 10)\n",
        "  test_labels = _one_hot(test_labels, 10)\n",
        "\n",
        "  if permute_train:\n",
        "    perm = np.random.RandomState(0).permutation(train_images.shape[0])\n",
        "    train_images = train_images[perm]\n",
        "    train_labels = train_labels[perm]\n",
        "\n",
        "  return train_images, train_labels, test_images, test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OofCYxeZziB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy.random as npr\n",
        "\n",
        "from jax.api import jit, grad\n",
        "from jax.config import config\n",
        "from jax.scipy.special import logsumexp\n",
        "import jax.numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)):\n",
        "  return [(scale * rng.randn(m, n), scale * rng.randn(n))\n",
        "          for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9niO-kh2zw6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(params, inputs):\n",
        "  activations = inputs\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = np.dot(activations, w) + b\n",
        "    activations = np.tanh(outputs)\n",
        "\n",
        "  final_w, final_b = params[-1]\n",
        "  logits = np.dot(activations, final_w) + final_b\n",
        "  return logits - logsumexp(logits, axis=1, keepdims=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSSxh7VLz0uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(params, batch):\n",
        "  inputs, targets = batch\n",
        "  preds = predict(params, inputs)\n",
        "  return -np.mean(np.sum(preds * targets, axis=1))\n",
        "\n",
        "def accuracy(params, batch):\n",
        "  inputs, targets = batch\n",
        "  target_class = np.argmax(targets, axis=1)\n",
        "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
        "  return np.mean(predicted_class == target_class)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-pkZ9o50QC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e4cb56f8-ba0a-4cdc-d4f0-6fe82adb9392"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  layer_sizes = [784, 1024, 1024, 10]\n",
        "  param_scale = 0.1\n",
        "  step_size = 0.001\n",
        "  num_epochs = 10\n",
        "  batch_size = 128\n",
        "\n",
        "  train_images, train_labels, test_images, test_labels = mnist()\n",
        "  num_train = train_images.shape[0]\n",
        "  num_complete_batches, leftover = divmod(num_train, batch_size)\n",
        "  num_batches = num_complete_batches + bool(leftover)\n",
        "\n",
        "  def data_stream():\n",
        "    rng = npr.RandomState(0)\n",
        "    while True:\n",
        "      perm = rng.permutation(num_train)\n",
        "      for i in range(num_batches):\n",
        "        batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
        "        yield train_images[batch_idx], train_labels[batch_idx]\n",
        "  batches = data_stream()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloaded https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz to /tmp/jax_example_data/\n",
            "downloaded https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz to /tmp/jax_example_data/\n",
            "downloaded https://storage.googleapis.com/cvdf-datasets/mnist/t10k-images-idx3-ubyte.gz to /tmp/jax_example_data/\n",
            "downloaded https://storage.googleapis.com/cvdf-datasets/mnist/t10k-labels-idx1-ubyte.gz to /tmp/jax_example_data/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/jax/lib/xla_bridge.py:118: UserWarning: No GPU/TPU found, falling back to CPU.\n",
            "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pNCV2bt2F-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  beta = 0.999\n",
        "  eps = 1e-08"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLqV7aAU0USN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  @jit\n",
        "  def update(params, batch):\n",
        "    grads = grad(loss)(params, batch)\n",
        "    #for (w, b), (dw, db) in zip(params, grads):\n",
        "      #v = np.zeros_like(w)\n",
        "      #v = beta * np.zeros_like(w) + (1 - beta) * (dw * dw)        # exponential weighted average\n",
        "      #return (w - step_size * dw / (np.sqrt(v) + eps), b - step_size * db)\n",
        "    return [(w - step_size * dw/(np.sqrt(beta * np.zeros_like(w) + (1 - beta) * (dw * dw))+eps), b - step_size * db)\n",
        "            for (w, b), (dw, db) in zip(params, grads)]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vvx8Woax0d9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "017e4c56-35d2-4058-ba6c-5b2724c21e4d"
      },
      "source": [
        "  params = init_random_params(param_scale, layer_sizes)\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for _ in range(num_batches):\n",
        "      params = update(params, next(batches))\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    train_acc = accuracy(params, (train_images, train_labels))\n",
        "    test_acc = accuracy(params, (test_images, test_labels))\n",
        "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "    print(\"Training set accuracy {}\".format(train_acc))\n",
        "    print(\"Test set accuracy {}\".format(test_acc))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 in 16.86 sec\n",
            "Training set accuracy 0.8787166476249695\n",
            "Test set accuracy 0.882099986076355\n",
            "Epoch 1 in 15.15 sec\n",
            "Training set accuracy 0.8690833449363708\n",
            "Test set accuracy 0.8639000058174133\n",
            "Epoch 2 in 15.05 sec\n",
            "Training set accuracy 0.8681333065032959\n",
            "Test set accuracy 0.864799976348877\n",
            "Epoch 3 in 14.90 sec\n",
            "Training set accuracy 0.8580333590507507\n",
            "Test set accuracy 0.8547999858856201\n",
            "Epoch 4 in 15.36 sec\n",
            "Training set accuracy 0.8741666674613953\n",
            "Test set accuracy 0.8762000203132629\n",
            "Epoch 5 in 15.26 sec\n",
            "Training set accuracy 0.9102166891098022\n",
            "Test set accuracy 0.9124000072479248\n",
            "Epoch 6 in 14.96 sec\n",
            "Training set accuracy 0.899649977684021\n",
            "Test set accuracy 0.9017999768257141\n",
            "Epoch 7 in 15.02 sec\n",
            "Training set accuracy 0.9016666412353516\n",
            "Test set accuracy 0.8992999792098999\n",
            "Epoch 8 in 15.09 sec\n",
            "Training set accuracy 0.9204166531562805\n",
            "Test set accuracy 0.9187999963760376\n",
            "Epoch 9 in 14.93 sec\n",
            "Training set accuracy 0.8982833623886108\n",
            "Test set accuracy 0.8984000086784363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2QcjKdZ3BBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}