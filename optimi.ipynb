{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rmsprop\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        cache = torch.zeros_like(p)\n",
    "        for p in model.parameters():\n",
    "          #rmsprop\n",
    "          eps = 1e-08\n",
    "          learning_rate = 0.01\n",
    "          \n",
    "          beta = 0.99\n",
    "          cache = beta * cache + (1 - beta) * (p.grad * p.grad)\n",
    "          p = p - 0.01*p.grad/(torch.sqrt(cache)+eps)\n",
    "     output = model(images)\n",
    "     loss = criterion(output, labels)\n",
    "     loss.backward()\n",
    "       \n",
    "        \n",
    "    running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam\n",
    "for e in range(epoochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        t = 0\n",
    "        m = torch.zeros_like(p)\n",
    "        cache = torch.zeros_like(p)\n",
    "        \n",
    "        for p in model.parameters():\n",
    "          #rmsprop\n",
    "          eps = 1e-08\n",
    "          learning_rate = 0.01\n",
    "          cache = torch.zeros_like(p)\n",
    "          beta1 = 0.99\n",
    "          beta2 = 0.999\n",
    "          t+=1\n",
    "          m = beta1 * m + (1 - beta1) * p.grad\n",
    "          mt = m / (1 - beta1**t) \n",
    "          cache = beta2 * cache + (1 - beta2) * (p.grad * p.grad)   # RMSprop\n",
    "          vt = v / (1 - beta2**t)                   # bias correction\n",
    "          p = p - learning_rate * mt / (torch.sqrt(vt) + eps)\n",
    "    output = model(images)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "       \n",
    "        \n",
    "    running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adagrad\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        for p in model.parameters():\n",
    "          #rmsprop\n",
    "          eps = 1e-08\n",
    "          learning_rate = 0.01\n",
    "          cache = torch.zeros_like(p)\n",
    "          beta = 0.99\n",
    "          cache = beta * cache + (1 - beta) * (p.grad * p.grad)\n",
    "          p = p - 0.01*p.grad/(torch.sqrt(cache)+eps)\n",
    "\n",
    "\n",
    "\n",
    "        # TODO: Training pass\n",
    "        \n",
    "        \n",
    "     output = model(images)\n",
    "     loss = criterion(output, labels)\n",
    "     loss.backward()\n",
    "       \n",
    "        \n",
    "     running_loss += loss.item()\n",
    "    else:\n",
    "     print(f\"Training loss: {running_loss/len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In 1st question I used pre made optimizer for optimizer.zero_grad() only. I manually updated the weights myself based on the optimiser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
